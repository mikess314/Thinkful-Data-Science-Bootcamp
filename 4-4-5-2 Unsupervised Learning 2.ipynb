{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import shakespeare, stopwords, gutenberg\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar = gutenberg.raw('shakespeare-caesar.txt')\n",
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "macbeth = gutenberg.raw('shakespeare-macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw:\n",
      " [The Tragedie of Julius Caesar by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Flauius, Murellus, and certaine Commoners ouer the Stage.\n",
      "\n",
      "  Flauius. Hence: home you idle Creatures, get you home:\n",
      "Is this a Holiday? What, know you not\n"
     ]
    }
   ],
   "source": [
    "print('\\nRaw:\\n', caesar[0:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the docs\n",
    "caesar = text_cleaner(caesar)\n",
    "hamlet = text_cleaner(hamlet)\n",
    "macbeth = text_cleaner(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw:\n",
      " Actus Primus. Scoena Prima. Enter Flauius, Murellus, and certaine Commoners ouer the Stage. Flauius. Hence: home you idle Creatures, get you home: Is this a Holiday? What, know you not (Being Mechanicall) you ought not walke Vpon a labouring day, wit\n"
     ]
    }
   ],
   "source": [
    "print('\\nRaw:\\n', caesar[0:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the docs\n",
    "nlp = spacy.load('en')\n",
    "caesar_doc = nlp(caesar)\n",
    "hamlet_doc = nlp(hamlet)\n",
    "macbeth_doc = nlp(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "caesar_sents = [[sent, \"Caesar\"] for sent in caesar_doc.sents]\n",
    "hamlet_sents = [[sent, \"Hamlet\"] for sent in hamlet_doc.sents]\n",
    "macbeth_sents = [[sent, \"MacBeth\"] for sent in macbeth_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Actus, Primus, .)</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Scoena, Prima, .)</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Enter, Flauius, ,, Murellus, ,, and, certaine...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Flauius, .)</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Hence, :, home, you, idle, Creatures, ,, get,...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0                                 (Actus, Primus, .)  Caesar\n",
       "1                                 (Scoena, Prima, .)  Caesar\n",
       "2  (Enter, Flauius, ,, Murellus, ,, and, certaine...  Caesar\n",
       "3                                       (Flauius, .)  Caesar\n",
       "4  (Hence, :, home, you, idle, Creatures, ,, get,...  Caesar"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pd.DataFrame(caesar_sents + hamlet_sents + macbeth_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar_words = bag_of_words(caesar_doc)\n",
    "hamlet_words = bag_of_words(hamlet_doc)\n",
    "macbeth_words = bag_of_words(macbeth_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(caesar_words + hamlet_words + macbeth_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n",
      "Processing row 800\n",
      "Processing row 850\n",
      "Processing row 900\n",
      "Processing row 950\n",
      "Processing row 1000\n",
      "Processing row 1050\n",
      "Processing row 1100\n",
      "Processing row 1150\n",
      "Processing row 1200\n",
      "Processing row 1250\n",
      "Processing row 1300\n",
      "Processing row 1350\n",
      "Processing row 1400\n",
      "Processing row 1450\n",
      "Processing row 1500\n",
      "Processing row 1550\n",
      "Processing row 1600\n",
      "Processing row 1650\n",
      "Processing row 1700\n",
      "Processing row 1750\n",
      "Processing row 1800\n",
      "Processing row 1850\n",
      "Processing row 1900\n",
      "Processing row 1950\n",
      "Processing row 2000\n",
      "Processing row 2050\n",
      "Processing row 2100\n",
      "Processing row 2150\n",
      "Processing row 2200\n",
      "Processing row 2250\n",
      "Processing row 2300\n",
      "Processing row 2350\n",
      "Processing row 2400\n",
      "Processing row 2450\n",
      "Processing row 2500\n",
      "Processing row 2550\n",
      "Processing row 2600\n",
      "Processing row 2650\n",
      "Processing row 2700\n",
      "Processing row 2750\n",
      "Processing row 2800\n",
      "Processing row 2850\n",
      "Processing row 2900\n",
      "Processing row 2950\n",
      "Processing row 3000\n",
      "Processing row 3050\n",
      "Processing row 3100\n",
      "Processing row 3150\n",
      "Processing row 3200\n",
      "Processing row 3250\n",
      "Processing row 3300\n",
      "Processing row 3350\n",
      "Processing row 3400\n",
      "Processing row 3450\n",
      "Processing row 3500\n",
      "Processing row 3550\n",
      "Processing row 3600\n",
      "Processing row 3650\n",
      "Processing row 3700\n",
      "Processing row 3750\n",
      "Processing row 3800\n",
      "Processing row 3850\n",
      "Processing row 3900\n",
      "Processing row 3950\n",
      "Processing row 4000\n",
      "Processing row 4050\n",
      "Processing row 4100\n",
      "Processing row 4150\n",
      "Processing row 4200\n",
      "Processing row 4250\n",
      "Processing row 4300\n",
      "Processing row 4350\n",
      "Processing row 4400\n",
      "Processing row 4450\n",
      "Processing row 4500\n",
      "Processing row 4550\n",
      "Processing row 4600\n",
      "Processing row 4650\n",
      "Processing row 4700\n",
      "Processing row 4750\n",
      "Processing row 4800\n",
      "Processing row 4850\n",
      "Processing row 4900\n",
      "Processing row 4950\n",
      "Processing row 5000\n",
      "Processing row 5050\n",
      "Processing row 5100\n",
      "Processing row 5150\n",
      "Processing row 5200\n",
      "Processing row 5250\n",
      "Processing row 5300\n",
      "Processing row 5350\n",
      "Processing row 5400\n",
      "Processing row 5450\n",
      "Processing row 5500\n",
      "Processing row 5550\n",
      "Processing row 5600\n",
      "Processing row 5650\n",
      "Processing row 5700\n",
      "Processing row 5750\n",
      "Processing row 5800\n",
      "Processing row 5850\n",
      "Processing row 5900\n",
      "Processing row 5950\n",
      "Processing row 6000\n",
      "Processing row 6050\n",
      "Processing row 6100\n",
      "Processing row 6150\n",
      "Processing row 6200\n",
      "Processing row 6250\n",
      "Processing row 6300\n",
      "Processing row 6350\n",
      "Processing row 6400\n",
      "Processing row 6450\n",
      "Processing row 6500\n",
      "Processing row 6550\n",
      "Processing row 6600\n",
      "Processing row 6650\n",
      "Processing row 6700\n",
      "Processing row 6750\n",
      "Processing row 6800\n",
      "Processing row 6850\n",
      "Processing row 6900\n",
      "Processing row 6950\n",
      "Processing row 7000\n",
      "Processing row 7050\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knottie</th>\n",
       "      <th>dishonourable</th>\n",
       "      <th>neu'r</th>\n",
       "      <th>of</th>\n",
       "      <th>briefe</th>\n",
       "      <th>rore</th>\n",
       "      <th>bar</th>\n",
       "      <th>expedition</th>\n",
       "      <th>meeting</th>\n",
       "      <th>lapt</th>\n",
       "      <th>...</th>\n",
       "      <th>fierie</th>\n",
       "      <th>brow</th>\n",
       "      <th>weaknesse</th>\n",
       "      <th>article</th>\n",
       "      <th>terror</th>\n",
       "      <th>naughty</th>\n",
       "      <th>stalke</th>\n",
       "      <th>vnmannerly</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Actus, Primus, .)</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Scoena, Prima, .)</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Enter, Flauius, ,, Murellus, ,, and, certaine...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Flauius, .)</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Hence, :, home, you, idle, Creatures, ,, get,...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4063 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  knottie dishonourable neu'r of briefe rore bar expedition meeting lapt  \\\n",
       "0       0             0     0  0      0    0   0          0       0    0   \n",
       "1       0             0     0  0      0    0   0          0       0    0   \n",
       "2       0             0     0  0      0    0   0          0       0    0   \n",
       "3       0             0     0  0      0    0   0          0       0    0   \n",
       "4       0             0     0  0      0    0   0          0       0    0   \n",
       "\n",
       "      ...     fierie brow weaknesse article terror naughty stalke vnmannerly  \\\n",
       "0     ...          0    0         0       0      0       0      0          0   \n",
       "1     ...          0    0         0       0      0       0      0          0   \n",
       "2     ...          0    0         0       0      0       0      0          0   \n",
       "3     ...          0    0         0       0      0       0      0          0   \n",
       "4     ...          0    0         0       0      0       0      0          0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0                                 (Actus, Primus, .)      Caesar  \n",
       "1                                 (Scoena, Prima, .)      Caesar  \n",
       "2  (Enter, Flauius, ,, Murellus, ,, and, certaine...      Caesar  \n",
       "3                                       (Flauius, .)      Caesar  \n",
       "4  (Hence, :, home, you, idle, Creatures, ,, get,...      Caesar  \n",
       "\n",
       "[5 rows x 4063 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and Y and the train, test split\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9574518100611189\n",
      "\n",
      "Test set score: 0.6826516220028209\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with every other corpus, the random forest suffers from gross overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4254, 4061) (4254,)\n",
      "Training set score: 0.8951574988246357\n",
      "\n",
      "Test set score: 0.7291960507757405\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is better than Random Forest, but still pretty bad. In these exercises, one model has always beaten them out. And that is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6817113305124589\n",
      "\n",
      "Test set score: 0.6734837799717912\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "clf1 = ensemble.GradientBoostingClassifier()\n",
    "train1 = clf1.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf1.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with every other one of these supervised learning methods for NLP, Gradient Boosting well outperforms the Logistic Progression and Random Forest. I'll tune this to maximize my score without overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7512929007992478\n",
      "\n",
      "Test set score: 0.7009873060648801\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting with double the n_estimators\n",
    "# This took several minutes\n",
    "\n",
    "clf2 = ensemble.GradientBoostingClassifier(n_estimators=200, max_depth=3)\n",
    "train2 = clf2.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf2.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7186177715091678\n",
      "\n",
      "Test set score: 0.688293370944993\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting with max_depth=4 instead of 3\n",
    "\n",
    "clf3 = ensemble.GradientBoostingClassifier(n_estimators=100, max_depth=4)\n",
    "train3 = clf3.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf3.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the increase in estimators had the better effect. increasing depth only added to the overfitting. Adding more estimators might increase it. But the processingn time will increase as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7837329572167372\n",
      "\n",
      "Test set score: 0.7165021156558533\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting with triple the n_estimators\n",
    "# This took several minutes\n",
    "\n",
    "clf4 = ensemble.GradientBoostingClassifier(n_estimators=300, max_depth=3)\n",
    "train4 = clf4.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf4.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8178185237423601\n",
      "\n",
      "Test set score: 0.7182651622002821\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting with quadruple the n_estimators\n",
    "# This took several minutes\n",
    "\n",
    "clf5 = ensemble.GradientBoostingClassifier(n_estimators=400, max_depth=3)\n",
    "train5 = clf5.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf5.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf5.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66725352, 0.67253521, 0.68838028, 0.68077601, 0.68849558])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clf2, X_test, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66901408, 0.67429577, 0.71126761, 0.6984127 , 0.70088496])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf4, X_test, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining which play among three plays written by the same author was always going to be a lower scoring model. However, the Gradient Boosting with a higher number of estimators seems to outperform the other models and yield good results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7090, 4063)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma=gutenberg.paras('austen-emma.txt')\n",
    "#processing\n",
    "emma_paras=[]\n",
    "for paragraph in emma:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    emma_paras.append(' '.join(para))\n",
    "\n",
    "print(emma_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar_p = gutenberg.paras('shakespeare-caesar.txt')\n",
    "macbeth_p = gutenberg.paras('shakespeare-macbeth.txt')\n",
    "hamlet_p = gutenberg.paras('shakespeare-hamlet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744\n",
      "678\n",
      "2372\n"
     ]
    }
   ],
   "source": [
    "print(len(caesar_p))\n",
    "print(len(macbeth_p))\n",
    "print(len(caesar_p + macbeth_p + hamlet_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Fla', '.'], ['But', 'wherefore', 'art', 'not', 'in', 'thy', 'Shop', 'to', 'day', '?'], ['Why', 'do', \"'\", 'st', 'thou', 'leade', 'these', 'men', 'about', 'the', 'streets', '?'], ['Cob', '.'], ['Truly', 'sir', ',', 'to', 'weare', 'out', 'their', 'shooes', ',', 'to', 'get', 'my', 'selfe', 'into', 'more', 'worke', '.'], ['But', 'indeede', 'sir', ',', 'we', 'make', 'Holyday', 'to', 'see', 'Caesar', ',', 'and', 'to', 'reioyce', 'in', 'his', 'Triumph']], [['Mur', '.'], ['Wherefore', 'reioyce', '?'], ['What', 'Conquest', 'brings', 'he', 'home', '?'], ['What', 'Tributaries', 'follow', 'him', 'to', 'Rome', ',', 'To', 'grace', 'in', 'Captiue', 'bonds', 'his', 'Chariot', 'Wheeles', '?'], ['You', 'Blockes', ',', 'you', 'stones', ',', 'you', 'worse', 'then', 'senslesse', 'things', ':', 'O', 'you', 'hard', 'hearts', ',', 'you', 'cruell', 'men', 'of', 'Rome', ',', 'Knew', 'you', 'not', 'Pompey', 'many', 'a', 'time', 'and', 'oft', '?'], ['Haue', 'you', 'climb', \"'\", 'd', 'vp', 'to', 'Walles', 'and', 'Battlements', ',', 'To', 'Towres', 'and', 'Windowes', '?'], ['Yea', ',', 'to', 'Chimney', 'tops', ',', 'Your', 'Infants', 'in', 'your', 'Armes', ',', 'and', 'there', 'haue', 'sate', 'The', 'liue', '-', 'long', 'day', ',', 'with', 'patient', 'expectation', ',', 'To', 'see', 'great', 'Pompey', 'passe', 'the', 'streets', 'of', 'Rome', ':', 'And', 'when', 'you', 'saw', 'his', 'Chariot', 'but', 'appeare', ',', 'Haue', 'you', 'not', 'made', 'an', 'Vniuersall', 'shout', ',', 'That', 'Tyber', 'trembled', 'vnderneath', 'her', 'bankes', 'To', 'heare', 'the', 'replication', 'of', 'your', 'sounds', ',', 'Made', 'in', 'her', 'Concaue', 'Shores', '?'], ['And', 'do', 'you', 'now', 'put', 'on', 'your', 'best', 'attyre', '?'], ['And', 'do', 'you', 'now', 'cull', 'out', 'a', 'Holyday', '?'], ['And', 'do', 'you', 'now', 'strew', 'Flowers', 'in', 'his', 'way', ',', 'That', 'comes', 'in', 'Triumph', 'ouer', 'Pompeyes', 'blood', '?'], ['Be', 'gone', ',', 'Runne', 'to', 'your', 'houses', ',', 'fall', 'vpon', 'your', 'knees', ',', 'Pray', 'to', 'the', 'Gods', 'to', 'intermit', 'the', 'plague', 'That', 'needs', 'must', 'light', 'on', 'this', 'Ingratitude']], ...]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caesar_p[10:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caesar_p[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mur', '.'],\n",
       " ['But', 'what', 'Trade', 'art', 'thou', '?'],\n",
       " ['Answer', 'me', 'directly']]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caesar_p[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important to note that the vectorizer has to iterate over a corpus, not a string, list, or even a parsed doc\n",
    "type(caesar_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar_paras = []\n",
    "macbeth_paras = []\n",
    "hamlet_paras = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in caesar_p:\n",
    "    para = paragraph[0]\n",
    "    caesar_paras.append(' '.join(para))\n",
    "    \n",
    "for paragraph in macbeth_p:\n",
    "    para = paragraph[0]\n",
    "    macbeth_paras.append(' '.join(para))\n",
    "\n",
    "for paragraph in hamlet_p:\n",
    "    para = paragraph[0]\n",
    "    hamlet_paras.append(' '.join(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ The Tragedie of Julius Caesar by William Shakespeare 1599 ]', 'Actus Primus .', 'Enter Flauius , Murellus , and certaine Commoners ouer the Stage .', 'Flauius .']\n",
      "['[ The Tragedie of Macbeth by William Shakespeare 1603 ]', 'Actus Primus .', 'Thunder and Lightning .', '1 .']\n",
      "['Ham .', 'Ham .', 'Ham .', 'Ham .', 'Ham .', 'Ham .', 'Ham .', 'Ham .', 'Hor .', 'Ham .']\n"
     ]
    }
   ],
   "source": [
    "print(caesar_paras[0:4])\n",
    "\n",
    "print(macbeth_paras[0:4])\n",
    "\n",
    "print(hamlet_paras[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(caesar_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_paras = list(caesar_paras+macbeth_paras+hamlet_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(caesar_paras, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.2, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 330\n"
     ]
    }
   ],
   "source": [
    "#Applying the vectorizer for all three docs\n",
    "s_paras_tfidf=vectorizer.fit_transform(s_paras)\n",
    "print(\"Number of features: %d\" % s_paras_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 138\n"
     ]
    }
   ],
   "source": [
    "#Applying the vectorizer for Ceasar\n",
    "caesar_paras_tfidf=vectorizer.fit_transform(caesar_paras)\n",
    "print(\"Number of features: %d\" % caesar_paras_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 109\n"
     ]
    }
   ],
   "source": [
    "#Applying the vectorizer for Macbeth\n",
    "macbeth_paras_tfidf=vectorizer.fit_transform(macbeth_paras)\n",
    "print(\"Number of features: %d\" % macbeth_paras_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 69\n"
     ]
    }
   ],
   "source": [
    "#Applying the vectorizer for Hamlet\n",
    "# Look how few features it generated compared to the others. \n",
    "hamlet_paras_tfidf=vectorizer.fit_transform(hamlet_paras)\n",
    "print(\"Number of features: %d\" % hamlet_paras_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the length of Hamlet\n",
    "len(hamlet_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WHich is longer than Caesar, which yielded more features than Hamlet. Interesting\n",
    "len(caesar_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(s_paras_tfidf, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of features\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Luc .\n",
      "Tf_idf vector: {'ham': 1.0}\n"
     ]
    }
   ],
   "source": [
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[50])\n",
    "print('Tf_idf vector:', tfidf_bypara[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe paragraphs are breaking up in to words, each with a vector of 1.0. Useless clearly. Maybe I'll try sentences. Clean them first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar = text_cleaner(caesar)\n",
    "hamlet = text_cleaner(hamlet)\n",
    "macbeth = text_cleaner(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(caesar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(caesar_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109364"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caesar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the docs\n",
    "nlp = spacy.load('en')\n",
    "caesar_doc = nlp(caesar)\n",
    "hamlet_doc = nlp(hamlet)\n",
    "macbeth_doc = nlp(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "caesar_sents = [[sent] for sent in caesar_doc.sents]\n",
    "hamlet_sents = [[sent] for sent in hamlet_doc.sents]\n",
    "macbeth_sents = [[sent] for sent in macbeth_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Actus Primus.],\n",
       " [Scoena Prima.],\n",
       " [Enter Flauius, Murellus, and certaine Commoners ouer the Stage.],\n",
       " [Flauius.],\n",
       " [Hence: home you idle Creatures, get you home:],\n",
       " [Is this a Holiday?],\n",
       " [What, know you not (Being Mechanicall)],\n",
       " [you ought not walke],\n",
       " [Vpon a labouring day, without the signe Of your Profession?],\n",
       " [Speake, what Trade art thou?]]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caesar_sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(caesar_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2144"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caesar_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-274d918a87f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcaesar_tfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaesar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of features: %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcaesar_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             raise ValueError(\n\u001b[1;32m--> 860\u001b[1;33m                 \u001b[1;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m                 \"string object received.\")\n\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "caesar_tfidf=vectorizer.fit_transform(caesar)\n",
    "print(\"Number of features: %d\" % caesar_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've started running into errors where my vectorizer won't iterate over the other forms of the docs. If this is the case, then this unsupervised learning method may not be effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar = gutenberg.raw('shakespeare-caesar.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(caesar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar_paras = []\n",
    "macbeth_paras = []\n",
    "hamlet_paras = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in caesar:\n",
    "    para = paragraph[0]\n",
    "    caesar_paras.append(' '.join(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'T', 'h', 'e', ' ', 'T', 'r', 'a', 'g', 'e']\n"
     ]
    }
   ],
   "source": [
    "print(caesar_paras[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the way the .paras function of the text doc right at the beginning is reading over it. Anything else isn't working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112310"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(caesar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Julius Caesar by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Flauius, Murellus, and certaine Commoners ouer the Stage.\n",
      "\n",
      "  Flauius. Hence: home you idle Creatures, get you home:\n",
      "Is this a Holiday? What, know you not\n",
      "(Being Mechanicall) you ought not walke\n",
      "Vpon a labouring day, without the signe\n",
      "Of your Profession? Speake, what Trade art thou?\n",
      "  Car. Why Sir, a Carpenter\n",
      "\n",
      "   Mur. Where is thy Leather Apron, and thy Rule?\n",
      "What dost thou with thy best Apparrell on?\n",
      "You sir, what Trade are you?\n",
      "  Cobl. Truely Sir, in respect of a fine Workman, I am\n",
      "but as you would say, a Cobler\n",
      "\n",
      "   Mur. But what Trade art thou? Answer me directly\n",
      "\n",
      "   Cob. A Trade Sir, that I hope I may vse, with a safe\n",
      "Conscience, which is indeed Sir, a Mender of bad soules\n",
      "\n",
      "   Fla. What Trade thou knaue? Thou naughty knaue,\n",
      "what Trade?\n",
      "  Cobl. Nay I beseech you Sir, be not out with me: yet\n",
      "if you be out Sir, I can mend you\n",
      "\n",
      "   Mur. What mean'st thou by that? Mend mee, thou\n",
      "sawcy Fellow?\n",
      "  Cob. Why sir, Cobble you\n",
      "\n",
      "   Fla. Thou art a Cobler, art thou?\n",
      "  Cob. Truly sir, all that I liue by, is with the Aule: I\n",
      "meddle with no Tradesmans matters, nor womens matters;\n",
      "but withal I am indeed Sir, a Surgeon to old shooes:\n",
      "when they are in great danger, I recouer them. As proper\n",
      "men as euer trod vpon Neats Leather, haue gone vpon\n",
      "my handy-worke\n",
      "\n",
      "   Fla. But wherefore art not in thy Shop to day?\n",
      "Why do'st thou leade these men about the streets?\n",
      "  Cob. Truly sir, to weare out their shooes, to get my\n",
      "selfe into more worke. But indeede sir, we make Holyday\n",
      "to see Caesar, and to reioyce in his Triumph\n",
      "\n",
      "   Mur. Wherefore reioyce?\n",
      "What Conquest brings he home?\n",
      "What Tributaries follow him to Rome,\n",
      "To grace in Captiue bonds his Chariot Wheeles?\n",
      "You Blockes, you stones, you worse then senslesse things:\n",
      "O you hard hearts, you cruell men of Rome,\n",
      "Knew you not Pompey many a time and oft?\n",
      "Haue you climb'd vp to Walles and Battlements,\n",
      "To Towres and Windowes? Yea, to Chimney tops,\n",
      "Your Infants in your Armes, and there haue sate\n",
      "The liue-long day, with patient expectation,\n",
      "To see great Pompey passe the streets of Rome:\n",
      "And when you saw his Chariot but appeare,\n",
      "Haue you not made an Vniuersall shout,\n",
      "That Tyber trembled vnderneath her bankes\n",
      "To heare the replication of your sounds,\n",
      "Made in her Concaue Shores?\n",
      "And do you now put on your best attyre?\n",
      "And do you now cull out a Holyday?\n",
      "And do you now strew Flowers in his way,\n",
      "That comes in Triumph ouer Pompeyes blood?\n",
      "Be gone,\n",
      "Runne to your houses, fall vpon your knees,\n",
      "Pray to the Gods to intermit the plague\n",
      "That needs must light on this Ingratitude\n",
      "\n",
      "   Fla. Go, go, good Countrymen, and for this fault\n",
      "Assemble all the poore men of your sort;\n",
      "Draw them to Tyber bankes, and weepe your teares\n",
      "Into the Channell, till the lowest streame\n",
      "Do kisse the most exalted Shores of all.\n",
      "\n",
      "Exeunt. all the Commoners.\n",
      "\n",
      "See where their basest mettle be not mou'd,\n",
      "They vanish tongue-tyed in their guiltinesse:\n",
      "Go you downe that way towards the Capitoll,\n",
      "This way will I: Disrobe the Images,\n",
      "If you do finde them deckt with Ceremonies\n",
      "\n",
      "   Mur. May we do so?\n",
      "You know it is the Feast of Lupercall\n",
      "\n",
      "   Fla. It is no matter, let no Images\n",
      "Be hung with Caesars Trophees: Ile about,\n",
      "And driue away the Vulgar from the streets;\n",
      "So do you too, where you perceiue them thicke.\n",
      "These growing Feathers, pluckt from Caesars wing,\n",
      "Will make him flye an ordinary pitch,\n",
      "Who else would soare aboue the view of men,\n",
      "And keepe vs all in seruile fearefulnesse.\n",
      "\n",
      "Exeunt.\n",
      "\n",
      "Enter Caesar, Antony for the Course, Calphurnia, Portia, Decius,\n",
      "Cicero,\n",
      "Brutus, Cassius, Caska, a Soothsayer: after them Murellus and\n",
      "Flauius.\n",
      "\n",
      "  Caes. Calphurnia\n",
      "\n",
      "   Cask. Peace ho, Caesar speakes\n",
      "\n",
      "   Caes. Calphurnia\n",
      "\n",
      "   Calp. Heere my Lord\n",
      "\n",
      "   Caes. Stand you directly in Antonio's way,\n",
      "When he doth run his course. Antonio\n",
      "\n",
      "   Ant. Cæsar, my Lord\n",
      "\n",
      "   Caes. Forget not in your speed Antonio,\n",
      "To touch Calphurnia: for our Elders say,\n",
      "The Barren touched in this holy chace,\n",
      "Shake off their sterrile curse\n",
      "\n",
      "   Ant. I shall remember,\n",
      "When Caesar sayes, Do this; it is perform'd\n",
      "\n",
      "   Caes. Set on, and leaue no Ceremony out\n",
      "\n",
      "   Sooth. Caesar\n",
      "\n",
      "   Caes. Ha? Who calles?\n",
      "  Cask. Bid euery noyse be still: peace yet againe\n",
      "\n",
      "   Caes. Who is it in the presse, that calles on me?\n",
      "I heare a Tongue shriller then all the Musicke\n",
      "Cry, Caesar: Speake, Caesar is turn'd to heare\n",
      "\n",
      "   Sooth. Beware the Ides of March\n",
      "\n",
      "   Caes. What man is that?\n",
      "  Br. A Sooth-sayer bids you beware the Ides of March\n",
      "  Caes. Set him before me, let me see his face\n",
      "\n",
      "   Cassi. Fellow, come from the throng, look vpon Caesar\n",
      "\n",
      "   Caes. What sayst thou to me now? Speak once againe,\n",
      "  Sooth. Beware the Ides of March\n",
      "\n",
      "   Caes. He is a Dreamer, let vs leaue him: Passe.\n",
      "\n",
      "Sennet\n",
      "\n",
      "Exeunt. Manet Brut. & Cass.\n",
      "\n",
      "  Cassi. Will you go see the order of the course?\n",
      "  Brut. Not I\n",
      "\n",
      "   Cassi. I pray you do\n",
      "\n",
      "   Brut. I am not Gamesom: I do lacke some part\n",
      "Of that quicke Spirit that is in Antony:\n",
      "Let me not hinder Cassius your desires;\n",
      "Ile leaue you\n",
      "\n",
      "   Cassi. Brutus, I do obserue you now of late:\n",
      "I haue not from your eyes, that gentlenesse\n",
      "And shew of Loue, as I was wont to haue:\n",
      "You beare too stubborne, and too strange a hand\n",
      "Ouer your Friend, that loues you\n",
      "\n",
      "   Bru. Cassius,\n",
      "Be not deceiu'd: If I haue veyl'd my looke,\n",
      "I turne the trouble of my Countenance\n",
      "Meerely vpon my selfe. Vexed I am\n",
      "Of late, with passions of some difference,\n",
      "Conceptions onely proper to my selfe,\n",
      "Which giue some soyle (perhaps) to my Behauiours:\n",
      "But let not therefore my good Friends be greeu'd\n",
      "(Among which number Cassius be you one)\n",
      "Nor construe any further my neglect,\n",
      "Then that poore Brutus with himselfe at warre,\n",
      "Forgets the shewes of Loue to other men\n",
      "\n",
      "   Cassi. Then Brutus, I haue much mistook your passion,\n",
      "By meanes whereof, this Brest of mine hath buried\n",
      "Thoughts of great value, worthy Cogitations.\n",
      "Tell me good Brutus, Can you see your face?\n",
      "  Brutus. No Cassius:\n",
      "For the eye sees not it selfe but by reflection,\n",
      "By some other things\n",
      "\n",
      "   Cassius. 'Tis iust,\n",
      "And it is very much lamented Brutus,\n",
      "That you haue no such Mirrors, as will turne\n",
      "Your hidden worthinesse into your eye,\n",
      "That you might see your shadow:\n",
      "I haue heard,\n",
      "Where many of the best respect in Rome,\n",
      "(Except immortall Caesar) speaking of Brutus,\n",
      "And groaning vnderneath this Ages yoake,\n",
      "Haue wish'd, that Noble Brutus had his eyes\n",
      "\n",
      "   Bru. Into what dangers, would you\n",
      "Leade me Cassius?\n",
      "That you would haue me seeke into my selfe,\n",
      "For that which is not in me?\n",
      "  Cas. Therefore good Brutus, be prepar'd to heare:\n",
      "And since you know, you cannot see your selfe\n",
      "So well as by Reflection; I your Glasse,\n",
      "Will modestly discouer to your selfe\n",
      "That of your selfe, which you yet know not of.\n",
      "And be not iealous on me, gentle Brutus:\n",
      "Were I a common Laughter, or did vse\n",
      "To stale with ordinary Oathes my loue\n",
      "To euery new Protester: if you know,\n",
      "That I do fawne on men, and hugge them hard,\n",
      "And after scandall them: Or if you know,\n",
      "That I professe my selfe in Banquetting\n",
      "To all the Rout, then hold me dangerous.\n",
      "\n",
      "Flourish, and Shout.\n",
      "\n",
      "  Bru. What meanes this Showting?\n",
      "I do feare, the People choose Caesar\n",
      "For their King\n",
      "\n",
      "   Cassi. I, do you feare it?\n",
      "Then must I thinke you would not haue it so\n",
      "\n",
      "   Bru. I would not Cassius, yet I loue him well:\n",
      "But wherefore do you hold me heere so long?\n",
      "What is it, that you would impart to me?\n",
      "If it be ought toward the generall good,\n",
      "Set Honor in one eye, and Death i'th other,\n",
      "And I will looke on both indifferently:\n",
      "For let the Gods so speed mee, as I loue\n",
      "The name of Honor, more then I feare death\n",
      "\n",
      "   Cassi. I know that vertue to be in you Brutus,\n",
      "As well as I do know your outward fauour.\n",
      "Well, Honor is the subiect of my Story:\n",
      "I cannot tell, what you and other men\n",
      "Thinke of this life: But for my single selfe,\n",
      "I had as liefe not be, as liue to be\n",
      "In awe of such a Thing, as I my selfe.\n",
      "I was borne free as Caesar, so were you,\n",
      "We both haue fed as well, and we can both\n",
      "Endure the Winters cold, as well as hee.\n",
      "For once, vpon a Rawe and Gustie day,\n",
      "The troubled Tyber, chafing with her Shores,\n",
      "Caesar saide to me, Dar'st thou Cassius now\n",
      "Leape in with me into this angry Flood,\n",
      "And swim to yonder Point? Vpon the word,\n",
      "Accoutred as I was, I plunged in,\n",
      "And bad him follow: so indeed he did.\n",
      "The Torrent roar'd, and we did buffet it\n",
      "With lusty Sinewes, throwing it aside,\n",
      "And stemming it with hearts of Controuersie.\n",
      "But ere we could arriue the Point propos'd,\n",
      "Caesar cride, Helpe me Cassius, or I sinke.\n",
      "I (as Aeneas, our great Ancestor,\n",
      "Did from the Flames of Troy, vpon his shoulder\n",
      "The old Anchyses beare) so, from the waues of Tyber\n",
      "Did I the tyred Caesar: And this Man,\n",
      "Is now become a God, and Cassius is\n",
      "A wretched Creature, and must bend his body,\n",
      "If Caesar carelesly but nod on him.\n",
      "He had a Feauer when he was in Spaine,\n",
      "And when the Fit was on him, I did marke\n",
      "How he did shake: Tis true, this God did shake,\n",
      "His Coward lippes did from their colour flye,\n",
      "And that same Eye, whose bend doth awe the World,\n",
      "Did loose his Lustre: I did heare him grone:\n",
      "I, and that Tongue of his, that bad the Romans\n",
      "Marke him, and write his Speeches in their Bookes,\n",
      "Alas, it cried, Giue me some drinke Titinius,\n",
      "As a sicke Girle: Ye Gods, it doth amaze me,\n",
      "A man of such a feeble temper should\n",
      "So get the start of the Maiesticke world,\n",
      "And beare the Palme alone.\n",
      "\n",
      "Shout. Flourish.\n",
      "\n",
      "  Bru. Another generall shout?\n",
      "I do beleeue, that these applauses are\n",
      "For some new Honors, that are heap'd on Caesar\n",
      "\n",
      "   Cassi. Why man, he doth bestride the narrow world\n",
      "Like a Colossus, and we petty men\n",
      "Walke vnder his huge legges, and peepe about\n",
      "To finde our selues dishonourable Graues.\n",
      "Men at sometime, are Masters of their Fates.\n",
      "The fault (deere Brutus) is not in our Starres,\n",
      "But in our Selues, that we are vnderlings.\n",
      "Brutus and Caesar: What should be in that Caesar?\n",
      "Why should that name be sounded more then yours\n",
      "Write them together: Yours, is as faire a Name:\n",
      "Sound them, it doth become the mouth aswell:\n",
      "Weigh them, it is as heauy: Coniure with 'em,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bru\n"
     ]
    }
   ],
   "source": [
    "print(caesar[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['[', 'The', 'Tragedie', 'of', 'Julius', 'Caesar', 'by', 'William', 'Shakespeare', '1599', ']']], [['Actus', 'Primus', '.'], ['Scoena', 'Prima', '.']], ...]\n"
     ]
    }
   ],
   "source": [
    "print(caesar_p[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(caesar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(caesar_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(caesar_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-2eb8884cc0c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcaesar_doc_tfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaesar_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of features: %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcaesar_doc_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "caesar_doc_tfidf=vectorizer.fit_transform(caesar_doc)\n",
    "print(\"Number of features: %d\" % caesar_doc_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.tokens.token.Token' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-45b7bd71657b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcaesar_doc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpara\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mcaesar_paras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'spacy.tokens.token.Token' object does not support indexing"
     ]
    }
   ],
   "source": [
    "caesar_paras = []\n",
    "\n",
    "for paragraph in caesar_doc:\n",
    "    para = paragraph[0]\n",
    "    caesar_paras.append(' '.join(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cevtorizer hasn't worked on anything else. THat's odd. It's only worked on a:\n",
    "\n",
    "- corpus .paras function (nltk.corpus.reader.util.StreamBackedCorpusView)\n",
    "- to a list of those paragraphs through a for loop (list)\n",
    "\n",
    "### TF-IDF is not working for these docs. It's just a bad fit. The supervised method is proving superior in this set of corpuses. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
